{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2d3a18-bfd2-4a7c-b98d-197ea4fd09de",
   "metadata": {},
   "source": [
    "### 2023.10.19 - Introduction to Transformers |Â Homework 1\n",
    "In this exercise, you will implement your own character-based Tokenizer as well as an Embedding layer from scratch.\n",
    "\n",
    "To make your tokenizer more robust add a special $<UNK>$ character to your vocabulary.\n",
    "If a token id is not found in the vocabulary, this character or its token id should be returned instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d0e24-537c-4c11-bb0b-36e89d209295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import List, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ae1d01c0-25ac-457a-89cf-c6eb63e9bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab: List[str]):\n",
    "        pass\n",
    "\n",
    "    def parse(self, input: str) -> List[str]:\n",
    "        \"\"\"Convert a string to a list of characters.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def encode(self, tokens: List[str]) -> List[int]:\n",
    "        \"\"\"Encode a list of tokens into their corresponding indices.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "        \"\"\"Decode a list of indices back into a string.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9ce8dc52-5b66-43ad-9771-d19a5fc17412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, n_embd: int, d_embd: int):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"Perform a lookup for the given indices in the embedding table.\"\"\"\n",
    "        pass\n",
    "        \n",
    "    def __call__(self, input: Tensor) -> Tensor:\n",
    "        return self.forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "019463a3-6e83-4737-8111-ce439c842afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list('') # define your vocabulary here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddffaec-fc98-4edd-a429-358cd8a8147b",
   "metadata": {},
   "source": [
    "### Solution Example Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bfee1c3a-3ada-4c11-b624-a8b9757a6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import List, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e1437978-616a-48be-bef7-8cd31a297905",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab: List[str]):\n",
    "        # Ensure the '<UNK>' token is in the vocabulary\n",
    "        if '<UNK>' not in vocab:\n",
    "            vocab.append('<UNK>')\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.token2idx = {token: idx for idx, token in enumerate(vocab)}\n",
    "        self.idx2token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "    def parse(self, input: str) -> List[str]:\n",
    "        \"\"\"Convert a string to a list of characters.\"\"\"\n",
    "        return list(input)\n",
    "\n",
    "    def encode(self, tokens: List[str]) -> List[int]:\n",
    "        \"\"\"Encode a list of tokens into their corresponding indices.\"\"\"\n",
    "        return [self.token2idx.get(token, self.token2idx.get('<UNK>', None)) for token in tokens]\n",
    "\n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "        \"\"\"Decode a list of indices back into a string.\"\"\"\n",
    "        return ''.join([self.idx2token.get(idx, '<UNK>') for idx in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0f7ab79b-11a6-4ea1-9ed9-1db50fb5e3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.parse: ['c', 'a', 't', 'e', 'r', 'p', 'i', 'l', 'l', 'a', 'r']\n",
      "tokenizer.encode: [2, 0, 19, 4, 17, 15, 8, 11, 11, 0, 17]\n",
      "tokenizer.decode: caterpillar\n",
      "tokenizer.encode/decode unknown: <UNK>\n",
      "tokenizer.decode out of bounds: <UNK>\n"
     ]
    }
   ],
   "source": [
    "vocab = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "tokenizer = Tokenizer(vocab)\n",
    "\n",
    "# Test parsing\n",
    "tokens = tokenizer.parse('caterpillar')\n",
    "print(f\"tokenizer.parse: {tokens}\")\n",
    "\n",
    "# Test encoding\n",
    "token_ids = tokenizer.encode(tokens)\n",
    "print(f\"tokenizer.encode: {token_ids}\")\n",
    "\n",
    "# Test decoding\n",
    "print(f\"tokenizer.decode: {tokenizer.decode(token_ids)}\")\n",
    "\n",
    "# Test <UNK>\n",
    "print(f\"tokenizer.encode/decode unknown: {tokenizer.decode(tokenizer.encode(['$']))}\")\n",
    "print(f\"tokenizer.decode out of bounds: {tokenizer.decode([100])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "55709c35-4e9d-46b4-8395-d510999fce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, n_embd: int, d_embd: int):\n",
    "        self.lookup = torch.randn(n_embd, d_embd)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"Perform a lookup for the given indices in the embedding table.\"\"\"\n",
    "        if input.max() >= self.lookup.size(0) or input.min() < 0:\n",
    "            raise ValueError(\"Input tensor contains invalid indices for lookup table.\")\n",
    "        return self.lookup[input,:]\n",
    "        \n",
    "    def __call__(self, input: Tensor) -> Tensor:\n",
    "        return self.forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "91777efe-8a4b-448a-a8f6-e1e7849f33a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (torch.Size([11])):\n",
      "tensor([ 2,  0, 19,  4, 17, 15,  8, 11, 11,  0, 17])\n",
      "\n",
      "embedding_layer result (torch.Size([11, 3])):\n",
      "tensor([[-0.4900, -1.4057, -0.5188],\n",
      "        [ 2.4123,  0.7004,  1.8279],\n",
      "        [ 0.2239,  0.3873, -1.4005],\n",
      "        [ 0.1093, -1.2144,  3.0684],\n",
      "        [-0.4954, -0.2158,  1.0090],\n",
      "        [-0.1661,  0.2986,  0.3961],\n",
      "        [ 0.3524,  0.7499,  0.7890],\n",
      "        [-0.4177,  0.8679,  1.6980],\n",
      "        [-0.4177,  0.8679,  1.6980],\n",
      "        [ 2.4123,  0.7004,  1.8279],\n",
      "        [-0.4954, -0.2158,  1.0090]])\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(vocab), 3)\n",
    "input = torch.tensor(tokenizer.encode(tokenizer.parse('caterpillar')))\n",
    "result = embedding_layer(input)\n",
    "\n",
    "print(f\"input ({input.size()}):\\n{input}\\n\")\n",
    "print(f\"embedding_layer result ({result.size()}):\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e7562-bb83-4dc3-bfaf-b4c56c361308",
   "metadata": {},
   "source": [
    "#### Solution Example Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883b66e-4dec-472a-abee-bc9f424661b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, n_embd: int, d_embd: int):\n",
    "        self.lookup = torch.randn(n_embd, d_embd, requires_grad=True)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"Perform a lookup for the given indices in the embedding table.\"\"\"\n",
    "        if input.max() >= self.lookup.size(0) or input.min() < 0:\n",
    "            raise ValueError(\"Input tensor contains invalid indices for lookup table.\")\n",
    "        return self.lookup[input,:]\n",
    "        \n",
    "    def __call__(self, input: Tensor) -> Tensor:\n",
    "        return self.forward(input)\n",
    "\n",
    "    def parameters(self) -> Iterator[Tensor]:\n",
    "        \"\"\"Return an iterator over the parameters.\"\"\"\n",
    "        yield self.lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9c4d821c-2a3a-4500-8a01-2cc7e1d2b6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Embedding.parameters at 0x169adef80>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "381e14de-5547-480e-9a1d-b3a5dc83dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(parameters, lr=0.01):\n",
    "    with torch.no_grad():\n",
    "        for param in parameters:\n",
    "            param -= lr * param.grad\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b1b4d512-ab4f-4d0d-9ff9-6884568af002",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = outputs.sum()\n",
    "loss.backward()\n",
    "sgd(embedding_layer.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
